{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "021d2c93",
   "metadata": {},
   "source": [
    "# Emotion Recognition using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a71f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f5b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Resize the image to match the input shape of the model\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "    # Normalize the image\n",
    "    normalized = resized / 255.0\n",
    "    # Reshape the image to match the input shape of the model\n",
    "    preprocessed = normalized.reshape(1, 48, 48, 1)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1999ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consts\n",
    "\n",
    "# Please provide you own path\n",
    "path = \"../../Datasets/fer2013/\"\n",
    "\n",
    "train_folder = path + \"train/\"\n",
    "valid_folder = path + \"test/\"\n",
    "\n",
    "emotion_labels = ['Happy', 'Surprise', 'Sad', 'Neutral', 'Angry', 'Disgust', 'Fear']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef796694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining emotions\n",
    "class Emotions:\n",
    "    def __init__(self):\n",
    "        self.emotion_labels = ['Happy', 'Surprise', 'Sad', 'Neutral', 'Angry', 'Disgust', 'Fear']\n",
    "\n",
    "    def get_emotion_label(self, index):\n",
    "        return self.emotion_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5bfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    preprocessed_images = []\n",
    "    for image in images:\n",
    "        # Resize the image to a desired size\n",
    "        resized_image = cv2.resize(image, (48, 48))\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Normalize the pixel values to the range [0, 1]\n",
    "        normalized_image = gray_image / 255.0\n",
    "        preprocessed_images.append(normalized_image)\n",
    "    return preprocessed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fba7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for emotion_idx, emotion in enumerate(emotion_labels):\n",
    "        folder_path = os.path.join(folder, emotion)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "                labels.append(emotion_idx)\n",
    "    # Preprocess the images\n",
    "    preprocessed_images = preprocess_images(images)\n",
    "    return preprocessed_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d5e892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from the train folder\n",
    "x_train, y_train = load_and_preprocess_images_from_folder(train_folder)\n",
    "\n",
    "# Load images from the validation folder\n",
    "x_valid, y_valid = load_and_preprocess_images_from_folder(valid_folder)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_valid = np.array(x_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# Reshape the input arrays to match the expected input shape of the model\n",
    "x_train = x_train.reshape(-1, 48, 48, 1)\n",
    "x_valid = x_valid.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "num_classes = len(emotion_labels)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67228ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 46, 46, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 23, 23, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 21, 21, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 2, 2, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 2, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 1, 1, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1, 1, 128)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1024)              132096    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 931,847\n",
      "Trainable params: 931,143\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(7, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7f3acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "449/449 [==============================] - 10s 13ms/step - loss: 1.8185 - accuracy: 0.2570 - val_loss: 1.7592 - val_accuracy: 0.2519\n",
      "Epoch 2/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 1.5736 - accuracy: 0.3796 - val_loss: 1.5007 - val_accuracy: 0.4533\n",
      "Epoch 3/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 1.4290 - accuracy: 0.4517 - val_loss: 1.4695 - val_accuracy: 0.4395\n",
      "Epoch 4/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 1.3494 - accuracy: 0.4889 - val_loss: 1.3406 - val_accuracy: 0.4799\n",
      "Epoch 5/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 1.2814 - accuracy: 0.5128 - val_loss: 1.3500 - val_accuracy: 0.4946\n",
      "Epoch 6/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 1.2359 - accuracy: 0.5328 - val_loss: 1.2638 - val_accuracy: 0.5212\n",
      "Epoch 7/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 1.1852 - accuracy: 0.5535 - val_loss: 1.2855 - val_accuracy: 0.5015\n",
      "Epoch 8/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 1.1454 - accuracy: 0.5694 - val_loss: 1.2514 - val_accuracy: 0.5190\n",
      "Epoch 9/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 1.0804 - accuracy: 0.5920 - val_loss: 1.1709 - val_accuracy: 0.5579\n",
      "Epoch 10/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 1.0414 - accuracy: 0.6122 - val_loss: 1.1896 - val_accuracy: 0.5486\n",
      "Epoch 11/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 1.0038 - accuracy: 0.6265 - val_loss: 1.2009 - val_accuracy: 0.5629\n",
      "Epoch 12/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.9536 - accuracy: 0.6470 - val_loss: 1.2428 - val_accuracy: 0.5507\n",
      "Epoch 13/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.9037 - accuracy: 0.6647 - val_loss: 1.2245 - val_accuracy: 0.5649\n",
      "Epoch 14/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.8669 - accuracy: 0.6801 - val_loss: 1.2545 - val_accuracy: 0.5559\n",
      "Epoch 15/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.8287 - accuracy: 0.6959 - val_loss: 1.2897 - val_accuracy: 0.5737\n",
      "Epoch 16/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.7890 - accuracy: 0.7112 - val_loss: 1.2479 - val_accuracy: 0.5861\n",
      "Epoch 17/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.7594 - accuracy: 0.7249 - val_loss: 1.3399 - val_accuracy: 0.5632\n",
      "Epoch 18/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.7266 - accuracy: 0.7361 - val_loss: 1.3733 - val_accuracy: 0.5574\n",
      "Epoch 19/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.6844 - accuracy: 0.7546 - val_loss: 1.4852 - val_accuracy: 0.5672\n",
      "Epoch 20/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.6501 - accuracy: 0.7710 - val_loss: 1.3983 - val_accuracy: 0.5697\n",
      "Epoch 21/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.6324 - accuracy: 0.7763 - val_loss: 1.4983 - val_accuracy: 0.5815\n",
      "Epoch 22/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.6122 - accuracy: 0.7845 - val_loss: 1.4225 - val_accuracy: 0.5816\n",
      "Epoch 23/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.5727 - accuracy: 0.7999 - val_loss: 1.5072 - val_accuracy: 0.5525\n",
      "Epoch 24/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.5475 - accuracy: 0.8079 - val_loss: 1.5697 - val_accuracy: 0.5693\n",
      "Epoch 25/64\n",
      "449/449 [==============================] - 6s 13ms/step - loss: 0.5319 - accuracy: 0.8164 - val_loss: 1.6132 - val_accuracy: 0.5802\n",
      "Epoch 26/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.5102 - accuracy: 0.8242 - val_loss: 1.6781 - val_accuracy: 0.5891\n",
      "Epoch 27/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.4901 - accuracy: 0.8314 - val_loss: 1.5746 - val_accuracy: 0.5808\n",
      "Epoch 28/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.4743 - accuracy: 0.8376 - val_loss: 1.6245 - val_accuracy: 0.5686\n",
      "Epoch 29/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.4585 - accuracy: 0.8453 - val_loss: 1.9459 - val_accuracy: 0.5580\n",
      "Epoch 30/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.4427 - accuracy: 0.8511 - val_loss: 1.7140 - val_accuracy: 0.5713\n",
      "Epoch 31/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.4178 - accuracy: 0.8588 - val_loss: 1.8007 - val_accuracy: 0.5787\n",
      "Epoch 32/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.4093 - accuracy: 0.8638 - val_loss: 1.8887 - val_accuracy: 0.5643\n",
      "Epoch 33/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.4035 - accuracy: 0.8627 - val_loss: 1.7721 - val_accuracy: 0.5591\n",
      "Epoch 34/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.3732 - accuracy: 0.8760 - val_loss: 1.9080 - val_accuracy: 0.5839\n",
      "Epoch 35/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.3723 - accuracy: 0.8785 - val_loss: 1.8650 - val_accuracy: 0.5809\n",
      "Epoch 36/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.3663 - accuracy: 0.8798 - val_loss: 1.8720 - val_accuracy: 0.5639\n",
      "Epoch 37/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.3486 - accuracy: 0.8865 - val_loss: 1.9180 - val_accuracy: 0.5840\n",
      "Epoch 38/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.3383 - accuracy: 0.8882 - val_loss: 2.0782 - val_accuracy: 0.5593\n",
      "Epoch 39/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.3406 - accuracy: 0.8893 - val_loss: 1.9818 - val_accuracy: 0.5632\n",
      "Epoch 40/64\n",
      "449/449 [==============================] - 6s 13ms/step - loss: 0.3206 - accuracy: 0.8963 - val_loss: 2.0655 - val_accuracy: 0.5707\n",
      "Epoch 41/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.3211 - accuracy: 0.8963 - val_loss: 2.0729 - val_accuracy: 0.5665\n",
      "Epoch 42/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.3169 - accuracy: 0.8965 - val_loss: 1.9870 - val_accuracy: 0.5695\n",
      "Epoch 43/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2996 - accuracy: 0.9034 - val_loss: 2.0404 - val_accuracy: 0.5851\n",
      "Epoch 44/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2811 - accuracy: 0.9096 - val_loss: 2.1408 - val_accuracy: 0.5632\n",
      "Epoch 45/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2889 - accuracy: 0.9077 - val_loss: 2.1838 - val_accuracy: 0.5546\n",
      "Epoch 46/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2915 - accuracy: 0.9065 - val_loss: 2.2980 - val_accuracy: 0.5864\n",
      "Epoch 47/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2649 - accuracy: 0.9153 - val_loss: 2.0411 - val_accuracy: 0.5812\n",
      "Epoch 48/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2659 - accuracy: 0.9151 - val_loss: 2.2519 - val_accuracy: 0.5830\n",
      "Epoch 49/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2672 - accuracy: 0.9169 - val_loss: 2.2985 - val_accuracy: 0.5793\n",
      "Epoch 50/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2597 - accuracy: 0.9187 - val_loss: 2.2046 - val_accuracy: 0.5779\n",
      "Epoch 51/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2482 - accuracy: 0.9224 - val_loss: 2.1455 - val_accuracy: 0.5558\n",
      "Epoch 52/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2392 - accuracy: 0.9252 - val_loss: 2.0841 - val_accuracy: 0.5748\n",
      "Epoch 53/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2431 - accuracy: 0.9239 - val_loss: 2.2903 - val_accuracy: 0.5886\n",
      "Epoch 54/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2475 - accuracy: 0.9243 - val_loss: 2.1644 - val_accuracy: 0.5651\n",
      "Epoch 55/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2122 - accuracy: 0.9342 - val_loss: 2.2621 - val_accuracy: 0.5651\n",
      "Epoch 56/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2246 - accuracy: 0.9313 - val_loss: 2.2692 - val_accuracy: 0.5753\n",
      "Epoch 57/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2182 - accuracy: 0.9313 - val_loss: 2.3463 - val_accuracy: 0.5664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2196 - accuracy: 0.9305 - val_loss: 2.2861 - val_accuracy: 0.5776\n",
      "Epoch 59/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2101 - accuracy: 0.9349 - val_loss: 2.4863 - val_accuracy: 0.5837\n",
      "Epoch 60/64\n",
      "449/449 [==============================] - 6s 12ms/step - loss: 0.2016 - accuracy: 0.9367 - val_loss: 2.3760 - val_accuracy: 0.5615\n",
      "Epoch 61/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2044 - accuracy: 0.9365 - val_loss: 2.5575 - val_accuracy: 0.5760\n",
      "Epoch 62/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2167 - accuracy: 0.9339 - val_loss: 2.3788 - val_accuracy: 0.5870\n",
      "Epoch 63/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2007 - accuracy: 0.9378 - val_loss: 2.8042 - val_accuracy: 0.5651\n",
      "Epoch 64/64\n",
      "449/449 [==============================] - 5s 12ms/step - loss: 0.2042 - accuracy: 0.9382 - val_loss: 2.3998 - val_accuracy: 0.5697\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=64, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e7b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"6464.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "228fc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(image):\n",
    "    # Preprocess the image\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    # Perform the prediction\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    # Get the predicted emotion label\n",
    "    predicted_label_index = np.argmax(predictions[0])\n",
    "    predicted_label = emotion_labels[predicted_label_index]\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04fc74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'C:\\\\Users\\\\mrsal\\\\OneDrive\\\\Desktop\\\\angry.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "preprocessed_image = preprocess_image(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bff965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 238ms/step\n",
      "Predicted Emotion: Angry\n"
     ]
    }
   ],
   "source": [
    "# Predict the emotion in the image\n",
    "predicted_emotion = predict_emotion(image)\n",
    "\n",
    "# Print the predicted emotion\n",
    "print(\"Predicted Emotion:\", predicted_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6db7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec1dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a9dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
